"""
citeflex/unified_router.py

Unified routing logic combining the best of CiteFlex Pro and Cite Fix Pro.

Version History:
    2025-12-06 16:00 V3.6: Added legal citation parser to recognize already-formatted
                           legal citations. Patterns: "Case v. Case, 388 U.S. 1 (1967)"
                           and UK neutral citations "[2024] UKSC 1". Properly formatted
                           legal citations now bypass legal.py database search.
    2025-12-06 15:30 V3.5: Added interview and letter citation parsers.
                           Interview triggers: "interview by", "interview with", "oral history"
                           Letter trigger: "Person X to Person Y, Date" pattern
                           Both types now bypass database search when parsed successfully.
    2025-12-06 13:45 V3.4: Added citation parser to extract metadata from already-formatted
                           citations. Reformats without database search when citation is complete.
                           Preserves authoritative content while applying consistent style.
    2025-12-06 12:25 V3.3: Added resolve_place fallback in _book_dict_to_metadata
                           to ensure publisher places appear even when APIs omit them
    2025-12-06 12:15 V3.2: Added Google Books, Open Library, Library of Congress to
                           get_multiple_citations() UI options using books.search_all_engines()
    2025-12-06 11:50 V3.1: CRITICAL FIX - Changed search_by_doi to get_by_id,
                           added famous papers cache to _route_journal,
                           added fallback DOI regex extraction to _route_url
    2025-12-06 V3.0: Switched to Claude API as primary AI router
    2025-12-05 13:15 V1.0: Initial unified router combining both systems
    2025-12-05 13:15 V1.1: Added Westlaw pattern, verified all medical .gov exclusions
    2025-12-05 20:30 V2.0: Moved to engines/ architecture (superlegal, books)
    2025-12-05 21:00 V2.1: Fixed get_multiple_citations to return 3-tuples
    2025-12-05 21:30 V2.2: Added URL/DOI handling to get_multiple_citations
    2025-12-05 22:30 V2.3: Added famous papers cache (10,000 most-cited papers)
    2025-12-05 23:00 V2.4: Added Gemini AI fallback for UNKNOWN queries
    2025-12-05 22:45 V2.4: Fixed UNKNOWN routing to search books first
    2025-12-06 V3.0: Switched to Claude API as primary AI router

KEY IMPROVEMENTS OVER ORIGINAL router.py:
1. Legal detection uses legal.is_legal_citation() which checks FAMOUS_CASES cache
   during detection (not just regex patterns that miss bare case names)
2. Legal extraction uses legal.extract_metadata() for cache + CourtListener API
3. Book search uses books.py's GoogleBooksAPI + OpenLibraryAPI with PUBLISHER_PLACE_MAP
4. Academic search uses CiteFlex Pro's parallel engine execution
5. Medical URL override prevents PubMed/NIH URLs from routing to government
6. Claude AI router for ambiguous queries with multi-option support

ARCHITECTURE:
- Wrapper classes convert legal.py/books.py dicts → CitationMetadata
- Parallel execution via ThreadPoolExecutor (12s timeout)
- Routing priority: Legal → URL handling → Parallel search → Fallback
"""

import re
from typing import Optional, Tuple, List
from concurrent.futures import ThreadPoolExecutor, as_completed, TimeoutError as FuturesTimeout

from models import CitationMetadata, CitationType
from config import NEWSPAPER_DOMAINS, GOV_AGENCY_MAP
from utils.type_detection import detect_type, DetectionResult, is_url
from utils.metadata_extraction import extract_by_type
from formatters.base import get_formatter

# Import CiteFlex Pro engines
from engines.academic import CrossrefEngine, OpenAlexEngine, SemanticScholarEngine, PubMedEngine
from engines.doi import extract_doi_from_url, is_academic_publisher_url
from engines.google_scholar import GoogleScholarEngine
from engines.arxiv import ArxivEngine
from engines.video import YouTubeEngine, VimeoEngine

# Import Cite Fix Pro modules (now in engines/)
from engines import legal
from engines import books
from engines.famous_papers import find_famous_paper

# Import Claude-first guess function
from routers.claude import guess_and_search

# =============================================================================
# AI ROUTER CONFIGURATION (Claude primary, Gemini fallback)
# =============================================================================

import os
AI_ROUTER = os.environ.get('AI_ROUTER', 'claude').lower()  # 'claude' or 'gemini'

# Try to import Claude router (primary)
try:
    from routers.claude import classify_with_claude, get_citation_options
    CLAUDE_AVAILABLE = True
except ImportError:
    CLAUDE_AVAILABLE = False
    print("[UnifiedRouter] Claude router not available")

# Try to import Gemini router (fallback)
try:
    from routers.gemini import classify_with_gemini
    GEMINI_AVAILABLE = True
except ImportError:
    GEMINI_AVAILABLE = False


def classify_with_ai(query: str) -> Tuple[CitationType, Optional[CitationMetadata]]:
    """
    Use configured AI router (Claude preferred, Gemini fallback).
    Returns (CitationType, optional metadata).
    """
    if AI_ROUTER == 'claude' and CLAUDE_AVAILABLE:
        return classify_with_claude(query)
    elif AI_ROUTER == 'gemini' and GEMINI_AVAILABLE:
        return classify_with_gemini(query)
    elif CLAUDE_AVAILABLE:
        return classify_with_claude(query)
    elif GEMINI_AVAILABLE:
        return classify_with_gemini(query)
    else:
        return CitationType.UNKNOWN, None


AI_AVAILABLE = CLAUDE_AVAILABLE or GEMINI_AVAILABLE
if not AI_AVAILABLE:
    print("[UnifiedRouter] No AI router available - UNKNOWN queries will use default routing")
else:
    active_router = "CLAUDE" if (AI_ROUTER == 'claude' and CLAUDE_AVAILABLE) or (not GEMINI_AVAILABLE and CLAUDE_AVAILABLE) else "GEMINI"
    print(f"[UnifiedRouter] Using {active_router} for AI classification")


# =============================================================================
# CONFIGURATION
# =============================================================================

PARALLEL_TIMEOUT = 12  # seconds
MAX_WORKERS = 6

# Medical domains that should NOT route to government engine
MEDICAL_DOMAINS = ['pubmed', 'ncbi.nlm.nih.gov', 'nih.gov/health', 'medlineplus']


# =============================================================================
# ENGINE INSTANCES (reused across requests)
# =============================================================================

_crossref = CrossrefEngine()
_openalex = OpenAlexEngine()
_semantic = SemanticScholarEngine()
_pubmed = PubMedEngine()
_google_scholar = GoogleScholarEngine()
_arxiv = ArxivEngine()
_youtube = YouTubeEngine()
_vimeo = VimeoEngine()


# =============================================================================
# WRAPPER: CONVERT SUPERLEGAL.PY DICT → CitationMetadata
# =============================================================================

def _legal_dict_to_metadata(data: dict, raw_source: str) -> Optional[CitationMetadata]:
    """Convert legal.py extract_metadata() dict to CitationMetadata."""
    if not data:
        return None
    
    return CitationMetadata(
        citation_type=CitationType.LEGAL,
        raw_source=raw_source,
        source_engine=data.get('source_engine', 'Legal Cache/CourtListener'),
        case_name=data.get('case_name', ''),
        citation=data.get('citation', ''),
        court=data.get('court', ''),
        year=data.get('year', ''),
        jurisdiction=data.get('jurisdiction', 'US'),
        neutral_citation=data.get('neutral_citation', ''),
        url=data.get('url', ''),
        raw_data=data
    )


# =============================================================================
# WRAPPER: CONVERT BOOKS.PY DICT → CitationMetadata
# =============================================================================

def _book_dict_to_metadata(data: dict, raw_source: str) -> Optional[CitationMetadata]:
    """Convert books.py result dict to CitationMetadata."""
    if not data:
        return None
    
    # Get place, with fallback to publisher lookup if missing
    place = data.get('place', '')
    publisher = data.get('publisher', '')
    if not place and publisher:
        place = books.resolve_place(publisher, '')
    
    return CitationMetadata(
        citation_type=CitationType.BOOK,
        raw_source=raw_source,
        source_engine=data.get('source_engine', 'Google Books/Open Library'),
        title=data.get('title', ''),
        authors=data.get('authors', []),
        year=data.get('year', ''),
        publisher=publisher,
        place=place,
        isbn=data.get('isbn', ''),
        raw_data=data
    )


# =============================================================================
# CITATION PARSER: Extract metadata from already-formatted citations
# =============================================================================

def parse_existing_citation(query: str) -> Optional[CitationMetadata]:
    """
    Parse an already-formatted citation to extract metadata.
    
    This allows CiteFlex to reformat citations without searching databases,
    preserving the user's authoritative content while applying style rules.
    
    Supports:
    - Legal: Case Name, Volume Reporter Page (Court Year).
    - Chicago journal: Author, "Title," Journal Vol, no. Issue (Year): Pages. DOI
    - Chicago book: Author, Title (Place: Publisher, Year).
    - Interview: Name, interview by author, Date.
    - Letter: Person X to Person Y, Date.
    - APA patterns
    - Citations with DOIs/URLs
    
    Returns CitationMetadata if parsing succeeds, None otherwise.
    """
    if not query or len(query) < 15:
        return None
    
    query = query.strip()
    
    # Try legal citation first (has distinctive patterns)
    meta = _parse_legal_citation(query)
    if meta and (meta.case_name and meta.year):
        return meta
    
    # Try interview pattern (has distinctive triggers)
    meta = _parse_interview_citation(query)
    if meta and _is_citation_complete(meta):
        return meta
    
    # Try letter pattern (Person X to Person Y, Date)
    meta = _parse_letter_citation(query)
    if meta and _is_citation_complete(meta):
        return meta
    
    # Try journal pattern (most common in academic work)
    meta = _parse_journal_citation(query)
    if meta and _is_citation_complete(meta):
        return meta
    
    # Try book pattern
    meta = _parse_book_citation(query)
    if meta and _is_citation_complete(meta):
        return meta
    
    # Try newspaper pattern
    meta = _parse_newspaper_citation(query)
    if meta and _is_citation_complete(meta):
        return meta
    
    return None


def _parse_legal_citation(query: str) -> Optional[CitationMetadata]:
    """
    Parse already-formatted legal citation.
    
    Patterns recognized:
    - US: Case Name, Volume Reporter Page (Court Year).
      e.g., Loving v. Virginia, 388 U.S. 1 (1967).
    - US Circuit: Case Name, Volume F.2d/F.3d Page (Circuit Year).
      e.g., Johnson v. Branch, 364 F.2d 177 (4th Cir. 1966).
    - US District: Case Name, Volume F. Supp. Page (District Year).
      e.g., Landman v. Royster, 333 F. Supp. 621 (E.D. Va. 1971).
    - UK: Case Name [Year] Court Number
      e.g., R v Brown [1994] 1 AC 212
    
    Returns CitationMetadata if parsing succeeds, None otherwise.
    """
    if not query or len(query) < 10:
        return None
    
    query = query.strip()
    
    # Must have "v." or "v " pattern (case name indicator)
    if not re.search(r'\s+v\.?\s+', query, re.IGNORECASE):
        # Also check for "In re" or "Ex parte" patterns
        if not re.search(r'^(In\s+re|Ex\s+parte|Matter\s+of)\s+', query, re.IGNORECASE):
            return None
    
    case_name = ''
    citation = ''
    court = ''
    year = ''
    
    # =========================================================================
    # Pattern 1: US Citations - Case Name, Citation (Court Year).
    # =========================================================================
    
    # Match: Case Name, Volume Reporter Page (Court? Year)
    # Examples:
    #   Loving v. Virginia, 388 U.S. 1 (1967)
    #   Johnson v. Branch, 364 F.2d 177 (4th Cir. 1966)
    #   Landman v. Royster, 333 F. Supp. 621 (E.D. Va. 1971)
    
    us_pattern = re.compile(
        r'^(.+?)\s*,\s*'                    # Case name (up to comma)
        r'(\d+\s+'                           # Volume number
        r'(?:U\.S\.|S\.\s*Ct\.|L\.\s*Ed\.|'  # Supreme Court reporters
        r'F\.(?:\d[a-z]*d?\s*)?|'            # Federal Reporter (F., F.2d, F.3d, F. 4th)
        r'F\.\s*Supp\.(?:\s*\d+[a-z]*)?|'   # F. Supp., F. Supp. 2d, F. Supp. 3d
        r'[A-Z]\.\d*[a-z]*)'                 # State reporters (A.2d, N.E.2d, etc.)
        r'\s*\d+)'                           # Page number
        r'\s*\(([^)]+)\)'                    # Parenthetical (Court Year)
    )
    
    match = us_pattern.match(query)
    if match:
        case_name = match.group(1).strip()
        citation = match.group(2).strip()
        paren_content = match.group(3).strip()
        
        # Parse parenthetical: could be just year "(1967)" or court + year "(4th Cir. 1966)"
        year_match = re.search(r'(\d{4})', paren_content)
        if year_match:
            year = year_match.group(1)
            court_part = paren_content[:year_match.start()].strip().rstrip(',')
            if court_part:
                court = court_part
            else:
                # Infer court from reporter
                if 'U.S.' in citation:
                    court = 'Supreme Court of the United States'
                elif 'S. Ct.' in citation:
                    court = 'Supreme Court of the United States'
        
        return CitationMetadata(
            citation_type=CitationType.LEGAL,
            raw_source=query,
            source_engine="Parsed from formatted citation",
            case_name=case_name,
            citation=citation,
            court=court,
            year=year,
            jurisdiction='US'
        )
    
    # =========================================================================
    # Pattern 2: UK Neutral Citations - Case Name [Year] Court Number
    # =========================================================================
    
    uk_pattern = re.compile(
        r'^(.+?)\s*'                         # Case name
        r'\[(\d{4})\]\s*'                    # [Year]
        r'(\w+(?:\s+\w+)?)\s*'               # Court code (UKSC, EWCA Civ, etc.)
        r'(\d+)'                              # Case number
    )
    
    match = uk_pattern.match(query)
    if match:
        case_name = match.group(1).strip().rstrip(',')
        year = match.group(2)
        court_code = match.group(3).strip()
        case_number = match.group(4)
        citation = f'[{year}] {court_code} {case_number}'
        
        # Map court codes
        uk_courts = {
            'UKSC': 'Supreme Court',
            'UKHL': 'House of Lords',
            'EWCA Civ': 'Court of Appeal (Civil)',
            'EWCA Crim': 'Court of Appeal (Criminal)',
            'EWHC': 'High Court',
        }
        court = uk_courts.get(court_code, court_code)
        
        return CitationMetadata(
            citation_type=CitationType.LEGAL,
            raw_source=query,
            source_engine="Parsed from formatted citation",
            case_name=case_name,
            citation=citation,
            court=court,
            year=year,
            jurisdiction='UK'
        )
    
    # =========================================================================
    # Pattern 3: Simpler fallback - just extract what we can
    # =========================================================================
    
    # Try to extract case name before comma
    parts = query.split(',', 1)
    if len(parts) >= 1:
        potential_name = parts[0].strip()
        if re.search(r'\s+v\.?\s+', potential_name, re.IGNORECASE) or \
           re.search(r'^(In\s+re|Ex\s+parte)', potential_name, re.IGNORECASE):
            case_name = potential_name
            
            # Try to find year in rest
            if len(parts) > 1:
                rest = parts[1]
                year_match = re.search(r'\((\d{4})\)', rest)
                if year_match:
                    year = year_match.group(1)
                
                # Try to extract citation (numbers + reporter)
                cit_match = re.search(r'(\d+\s+[A-Z][A-Za-z\.\s]+\d+)', rest)
                if cit_match:
                    citation = cit_match.group(1).strip()
            
            if case_name and year:
                return CitationMetadata(
                    citation_type=CitationType.LEGAL,
                    raw_source=query,
                    source_engine="Parsed from formatted citation",
                    case_name=case_name,
                    citation=citation,
                    court=court,
                    year=year,
                    jurisdiction='US'
                )
    
    return None


def _parse_interview_citation(query: str) -> Optional[CitationMetadata]:
    """
    Parse interview citation.
    
    Triggers (high confidence):
    - "interview by" 
    - "interview with"
    - "oral history"
    - "interviewed by"
    
    Patterns:
    - Name, interview by author, Date, Location.
    - Name, interview with Author, Date.
    - Name interview by Author, Date. Digitally recorded in author's possession.
    """
    query_lower = query.lower()
    
    # Check for interview triggers
    triggers = ['interview by', 'interview with', 'oral history', 'interviewed by']
    has_trigger = any(t in query_lower for t in triggers)
    
    if not has_trigger:
        return None
    
    interviewee = ''
    interviewer = ''
    date = ''
    location = ''
    url = ''
    
    # Extract URL if present
    url_match = re.search(r'https?://[^\s,]+', query)
    if url_match:
        url = url_match.group(0).rstrip('.,;')
    
    # Pattern: Interviewee, interview by Interviewer, Date
    # Or: Interviewee interview by Interviewer, Date
    interview_match = re.search(
        r'^([^,]+?)(?:,\s*)?\binterview(?:ed)?\s+(?:by|with)\s+([^,]+)',
        query, re.IGNORECASE
    )
    
    if interview_match:
        interviewee = interview_match.group(1).strip()
        interviewer = interview_match.group(2).strip()
        
        # Get rest of string for date/location
        rest = query[interview_match.end():].strip().lstrip(',').strip()
        
        # Extract date (various formats)
        date_patterns = [
            r'([A-Z][a-z]+\.?\s+\d{1,2},?\s+\d{4})',  # Jan. 15, 2022
            r'(\d{1,2}\s+[A-Z][a-z]+\.?\s+\d{4})',     # 15 Jan 2022
            r'([A-Z][a-z]+\s+\d{4})',                   # January 2022
            r'(\d{4})',                                  # Just year
        ]
        
        for pattern in date_patterns:
            date_match = re.search(pattern, rest)
            if date_match:
                date = date_match.group(1)
                break
        
        # Location might be city name before or after date
        # Often: "Potomac, MD" or just location info
        loc_match = re.search(r',\s*([A-Z][a-z]+(?:,\s*[A-Z]{2})?)\s*,', rest)
        if loc_match:
            location = loc_match.group(1)
    
    # Handle oral history pattern
    elif 'oral history' in query_lower:
        # Pattern: Name Oral History Interview, Source
        oral_match = re.search(r'^([^,]+?)\s+oral\s+history', query, re.IGNORECASE)
        if oral_match:
            interviewee = oral_match.group(1).strip()
    
    if not interviewee:
        return None
    
    # Extract year from date
    year = ''
    if date:
        year_match = re.search(r'(\d{4})', date)
        if year_match:
            year = year_match.group(1)
    
    return CitationMetadata(
        citation_type=CitationType.INTERVIEW,
        raw_source=query,
        source_engine="Parsed from formatted citation",
        interviewee=interviewee,
        interviewer=interviewer,
        date=date,
        year=year,
        location=location,
        url=url
    )


def _parse_letter_citation(query: str) -> Optional[CitationMetadata]:
    """
    Parse letter/correspondence citation.
    
    Trigger: "Person X to Person Y, Date" pattern
    The date requirement prevents false positives like "Introduction to Psychology"
    
    Patterns:
    - John Grad to Philip J. Hirschkop, Apr. 19, 1977.
    - Aaron Fodiman to Henry Kissinger, Mar. 11, 1976.
    - Name to Name, "Subject," Date, Collection.
    """
    # Pattern: Name to Name, followed by a date
    # Must have date within reasonable distance to avoid "Introduction to X"
    
    # Look for "Name to Name" followed by comma and date-like content
    # Names typically: First Last or First M. Last or First Middle Last
    letter_pattern = re.compile(
        r'^([A-Z][a-z]+(?:\s+[A-Z]\.?)?\s+[A-Z][a-z]+(?:\s+[A-Z][a-z]+)?)'  # Sender
        r'\s+to\s+'
        r'([A-Z][a-z]+(?:\s+[A-Z]\.?)?\s+[A-Z][a-z]+(?:\s+[A-Z][a-z]+)?)'  # Recipient
        r',\s*'
        r'(.+)$',  # Rest (should contain date)
        re.MULTILINE
    )
    
    match = letter_pattern.match(query)
    if not match:
        return None
    
    sender = match.group(1).strip()
    recipient = match.group(2).strip()
    rest = match.group(3).strip()
    
    # Must have a date within the rest to confirm this is a letter
    date_patterns = [
        r'([A-Z][a-z]{2,8}\.?\s+\d{1,2},?\s+\d{4})',  # Apr. 19, 1977 or April 19, 1977
        r'(\d{1,2}\s+[A-Z][a-z]{2,8}\.?\s+\d{4})',     # 19 Apr 1977
        r'([A-Z][a-z]{2,8}\.?\s+\d{4})',               # April 1977
    ]
    
    date = ''
    year = ''
    for pattern in date_patterns:
        date_match = re.search(pattern, rest)
        if date_match:
            date = date_match.group(1)
            year_match = re.search(r'(\d{4})', date)
            if year_match:
                year = year_match.group(1)
            break
    
    # If no date found, this might not be a letter (could be "Introduction to Psychology")
    if not date:
        return None
    
    # Extract URL if present
    url = ''
    url_match = re.search(r'https?://[^\s,]+', rest)
    if url_match:
        url = url_match.group(0).rstrip('.,;')
    
    # Extract subject/title if in quotes
    title = ''
    title_match = re.search(r'"([^"]+)"', rest)
    if title_match:
        title = title_match.group(1)
    
    # Extract location/collection info (often after date)
    location = ''
    # Look for collection info after date
    if date:
        after_date_idx = rest.find(date) + len(date)
        after_date = rest[after_date_idx:].strip().lstrip(',').strip()
        # Remove URL from consideration
        if url:
            after_date = after_date.replace(url, '').strip()
        after_date = after_date.rstrip('.')
        if after_date and not after_date.startswith('http'):
            location = after_date
    
    return CitationMetadata(
        citation_type=CitationType.LETTER,
        raw_source=query,
        source_engine="Parsed from formatted citation",
        sender=sender,
        recipient=recipient,
        title=title,  # Subject line if present
        date=date,
        year=year,
        location=location,  # Collection/archive info
        url=url
    )


def _parse_journal_citation(query: str) -> Optional[CitationMetadata]:
    """
    Parse Chicago/academic journal citation.
    
    Patterns recognized:
    - Author, "Title," Journal Vol, no. Issue (Year): Pages. DOI
    - Author, "Title," Journal Vol (Year): Pages.
    - Author "Title," Journal Vol, no. Issue (Year): Pages DOI
    """
    # Extract DOI if present (do this first, then remove from parsing)
    doi = None
    doi_match = re.search(r'(?:https?://)?(?:dx\.)?doi\.org/([^\s,\.]+[^\s,\.\)])', query)
    if doi_match:
        doi = doi_match.group(1).rstrip('.')
    
    # Extract URL if present (and no DOI)
    url = None
    if not doi:
        url_match = re.search(r'https?://[^\s,]+', query)
        if url_match:
            url = url_match.group(0).rstrip('.,;')
    
    # Pattern: Author(s), "Title," Journal Volume, no. Issue (Year): Pages
    # Also handles: Author(s) "Title," (without comma after author)
    
    # Look for quoted title (strong indicator of journal/article)
    title_match = re.search(r'[,\s]"([^"]+)"[,\.]?\s*', query)
    if not title_match:
        # Try single quotes
        title_match = re.search(r"[,\s]'([^']+)'[,\.]?\s*", query)
    
    if not title_match:
        return None
    
    title = title_match.group(1).strip()
    before_title = query[:title_match.start()].strip().rstrip(',')
    after_title = query[title_match.end():].strip()
    
    # Parse authors (everything before the title)
    authors = _parse_authors(before_title)
    
    # Parse journal info from after title
    # Pattern: Journal Vol, no. Issue (Year): Pages
    # Or: Journal Vol (Year): Pages
    # Or: Journal Volume, no. Issue (Year) : Pages
    
    journal = None
    volume = None
    issue = None
    year = None
    pages = None
    
    # Extract year from parentheses
    year_match = re.search(r'\((\d{4})\)', after_title)
    if year_match:
        year = year_match.group(1)
    
    # Extract pages (after colon or before DOI)
    pages_match = re.search(r':\s*(\d+[-–]\d+|\d+)', after_title)
    if pages_match:
        pages = pages_match.group(1).replace('–', '-')
    
    # Extract volume and issue
    # Pattern: Vol, no. Issue or Volume no. Issue or Vol(Issue)
    vol_issue_match = re.search(r'(\d+)\s*,?\s*no\.?\s*(\d+)', after_title, re.IGNORECASE)
    if vol_issue_match:
        volume = vol_issue_match.group(1)
        issue = vol_issue_match.group(2)
    else:
        # Just volume
        vol_match = re.search(r'\b(\d+)\s*\(', after_title)
        if vol_match:
            volume = vol_match.group(1)
    
    # Extract journal name (text before volume/year, after title)
    # Remove DOI/URL from consideration
    journal_text = after_title
    if doi_match:
        journal_text = journal_text[:journal_text.find('doi.org') if 'doi.org' in journal_text.lower() else len(journal_text)]
    if url:
        journal_text = journal_text.replace(url, '')
    
    # Journal is typically italic in source, may have <i> tags
    # Pattern: <i>Journal Name</i> or just Journal Name before volume
    italic_match = re.search(r'<i>([^<]+)</i>', journal_text)
    if italic_match:
        journal = italic_match.group(1).strip()
    else:
        # Extract text before volume number
        if volume:
            vol_pos = journal_text.find(volume)
            if vol_pos > 0:
                journal = journal_text[:vol_pos].strip().rstrip(',').strip()
        elif year_match:
            year_pos = journal_text.find(f'({year})')
            if year_pos > 0:
                journal = journal_text[:year_pos].strip().rstrip(',').strip()
    
    # Clean up journal name
    if journal:
        journal = re.sub(r'^[,\s]+', '', journal)
        journal = re.sub(r'[,\s]+$', '', journal)
        # Remove any remaining HTML tags
        journal = re.sub(r'<[^>]+>', '', journal)
    
    if not title or not year:
        return None
    
    return CitationMetadata(
        citation_type=CitationType.JOURNAL,
        raw_source=query,
        source_engine="Parsed from formatted citation",
        title=title,
        authors=authors if authors else [],
        journal=journal or '',
        volume=volume or '',
        issue=issue or '',
        year=year,
        pages=pages or '',
        doi=doi or '',
        url=url or ''
    )


def _parse_book_citation(query: str) -> Optional[CitationMetadata]:
    """
    Parse Chicago book citation.
    
    Patterns recognized:
    - Author, Title (Place: Publisher, Year).
    - Author, Title (Publisher, Year).
    - Author. Title. Place: Publisher, Year.
    """
    # Books have italic titles (not quoted)
    # Look for (Place: Publisher, Year) or (Publisher, Year) pattern
    
    pub_match = re.search(r'\(([^)]+:\s*[^,)]+,\s*\d{4})\)', query)
    if not pub_match:
        # Try (Publisher, Year) without place
        pub_match = re.search(r'\(([^):,]+,\s*\d{4})\)', query)
    
    if not pub_match:
        return None
    
    pub_info = pub_match.group(1)
    before_pub = query[:pub_match.start()].strip()
    
    # Parse publication info
    place = ''
    publisher = ''
    year = ''
    
    # Extract year
    year_match = re.search(r'(\d{4})', pub_info)
    if year_match:
        year = year_match.group(1)
    
    # Check for Place: Publisher pattern
    if ':' in pub_info:
        parts = pub_info.split(':')
        place = parts[0].strip()
        rest = ':'.join(parts[1:])
        # Publisher is before the year
        publisher = re.sub(r',?\s*\d{4}', '', rest).strip().rstrip(',')
    else:
        # Just Publisher, Year
        publisher = re.sub(r',?\s*\d{4}', '', pub_info).strip().rstrip(',')
    
    # Parse author and title from before_pub
    # Pattern: Author, Title or Author. Title.
    
    # Look for italic title marker
    italic_match = re.search(r'<i>([^<]+)</i>', before_pub)
    if italic_match:
        title = italic_match.group(1).strip()
        before_title = before_pub[:italic_match.start()].strip().rstrip(',').rstrip('.')
        authors = _parse_authors(before_title)
    else:
        # No italic markers - try to split on comma after author name pattern
        # Author names typically end before a capitalized title
        # Heuristic: first comma after a name-like pattern
        
        # Simple split: everything before last comma-space before title
        # For "John Smith, The Great Book" -> author="John Smith", title="The Great Book"
        
        # Try to find where title starts (usually capitalized, might have subtitle with colon)
        parts = before_pub.split(', ')
        if len(parts) >= 2:
            # Assume first part(s) are author, last significant part is title
            # Look for title-like part (longer, has capitalized words)
            author_parts = []
            title = ''
            for i, part in enumerate(parts):
                # If part looks like a title (longer, not a name pattern)
                if len(part) > 30 or (i > 0 and ':' in part):
                    title = ', '.join(parts[i:])
                    break
                author_parts.append(part)
            
            if not title and len(parts) >= 2:
                author_parts = parts[:-1]
                title = parts[-1]
            
            authors = _parse_authors(', '.join(author_parts))
        else:
            # Can't reliably split
            authors = []
            title = before_pub
    
    if not title or not year:
        return None
    
    # Clean title
    title = re.sub(r'<[^>]+>', '', title).strip()
    
    return CitationMetadata(
        citation_type=CitationType.BOOK,
        raw_source=query,
        source_engine="Parsed from formatted citation",
        title=title,
        authors=authors if authors else [],
        publisher=publisher,
        place=place,
        year=year
    )


def _parse_newspaper_citation(query: str) -> Optional[CitationMetadata]:
    """
    Parse newspaper article citation.
    
    Pattern: Author, "Title," Publication, Date, URL.
    """
    # Must have quoted title and a URL or date
    title_match = re.search(r'"([^"]+)"', query)
    if not title_match:
        return None
    
    title = title_match.group(1)
    before_title = query[:title_match.start()].strip().rstrip(',')
    after_title = query[title_match.end():].strip().lstrip(',').strip()
    
    # Extract URL
    url = None
    url_match = re.search(r'https?://[^\s,]+', after_title)
    if url_match:
        url = url_match.group(0).rstrip('.,;')
        after_title = after_title.replace(url, '').strip()
    
    # Parse authors
    authors = _parse_authors(before_title)
    
    # After title: Publication, Date
    # Look for italic publication name
    pub_match = re.search(r'<i>([^<]+)</i>', after_title)
    newspaper = ''
    date = ''
    
    if pub_match:
        newspaper = pub_match.group(1).strip()
        rest = after_title[pub_match.end():].strip().lstrip(',').strip()
        # Rest might be date
        date_match = re.search(r'([A-Z][a-z]+\.?\s+\d{1,2},?\s+\d{4}|\d{4})', rest)
        if date_match:
            date = date_match.group(1)
    else:
        # Try to extract date and newspaper from remaining text
        # Common patterns: New York Times, July 9, 2025
        parts = [p.strip() for p in after_title.split(',')]
        for i, part in enumerate(parts):
            if re.search(r'\d{4}', part):
                date = ', '.join(parts[i:]).strip().rstrip('.')
                newspaper = ', '.join(parts[:i]).strip() if i > 0 else ''
                break
    
    if not title:
        return None
    
    # Extract year from date
    year = ''
    year_match = re.search(r'(\d{4})', date or '')
    if year_match:
        year = year_match.group(1)
    
    return CitationMetadata(
        citation_type=CitationType.NEWSPAPER,
        raw_source=query,
        source_engine="Parsed from formatted citation",
        title=title,
        authors=authors if authors else [],
        newspaper=newspaper,
        date=date,
        year=year,
        url=url or ''
    )


def _parse_authors(author_str: str) -> list:
    """
    Parse author string into list of names.
    
    Handles:
    - Single author: "John Smith"
    - Two authors: "John Smith and Jane Doe"
    - Multiple: "John Smith, Jane Doe, and Bob Wilson"
    - Et al: "John Smith et al."
    """
    if not author_str:
        return []
    
    author_str = author_str.strip()
    
    # Remove trailing punctuation
    author_str = author_str.rstrip('.,;:')
    
    # Handle et al.
    if 'et al' in author_str.lower():
        # Just get first author
        first = re.split(r'\s+et\s+al', author_str, flags=re.IGNORECASE)[0]
        return [first.strip().rstrip(',')]
    
    # Split on " and " or ", and "
    if ' and ' in author_str:
        parts = re.split(r',?\s+and\s+', author_str)
        authors = []
        for part in parts:
            # Further split on commas (for lists like "A, B, and C")
            sub_parts = [p.strip() for p in part.split(',') if p.strip()]
            authors.extend(sub_parts)
        return authors
    
    # Check if comma-separated (multiple authors)
    # But be careful: "Smith, John" is one author in Last, First format
    parts = [p.strip() for p in author_str.split(',')]
    if len(parts) == 2 and len(parts[1].split()) <= 2:
        # Likely "Last, First" format - single author
        return [author_str]
    elif len(parts) > 2:
        # Multiple authors
        return parts
    
    return [author_str]


def _is_citation_complete(meta: CitationMetadata) -> bool:
    """
    Check if parsed citation has enough data to skip database search.
    
    Criteria:
    - Journal: title + (journal OR year)
    - Book: title + (publisher OR year)  
    - Newspaper: title + (newspaper OR date OR url)
    - Interview: interviewee + (date OR interviewer)
    - Letter: sender + recipient + date
    - Legal: handled separately (not parsed here)
    """
    if not meta:
        return False
    
    if meta.citation_type == CitationType.INTERVIEW:
        # Need interviewee plus date or interviewer
        return bool(meta.interviewee and (meta.date or meta.interviewer))
    
    elif meta.citation_type == CitationType.LETTER:
        # Need sender, recipient, and date
        return bool(meta.sender and meta.recipient and meta.date)
    
    # For other types, title is required
    if not meta.title:
        return False
    
    if meta.citation_type == CitationType.JOURNAL:
        # Need title plus journal name or year
        return bool(meta.journal or meta.year)
    
    elif meta.citation_type == CitationType.BOOK:
        # Need title plus publisher or year
        return bool(meta.publisher or meta.year)
    
    elif meta.citation_type == CitationType.NEWSPAPER:
        # Need title plus newspaper name or date or URL
        return bool(meta.newspaper or meta.date or meta.url)
    
    return False


# =============================================================================
# UNIFIED LEGAL SEARCH (uses legal.py)
# =============================================================================

def _route_legal(query: str) -> Optional[CitationMetadata]:
    """
    Route legal case queries using Cite Fix Pro's legal.py.
    
    This is superior to CiteFlex Pro's legal.py because:
    1. FAMOUS_CASES cache has 100+ landmark cases
    2. is_legal_citation() checks cache during detection (catches "Roe v Wade")
    3. Fuzzy matching via difflib for near-matches
    4. CourtListener API fallback with phrase/keyword/fuzzy attempts
    """
    try:
        data = legal.extract_metadata(query)
        if data and (data.get('case_name') or data.get('citation')):
            return _legal_dict_to_metadata(data, query)
    except Exception as e:
        print(f"[UnifiedRouter] Legal search error: {e}")
    
    return None


# =============================================================================
# UNIFIED BOOK SEARCH (uses books.py)
# =============================================================================

def _validate_book_match(query: str, book_dict: dict) -> bool:
    """
    Validate that a book result actually matches the search query.
    
    Prevents returning wrong books like "Tackle Football..." for "Caplan trains brains".
    
    Requirements:
    1. At least 2 meaningful words from query must appear in title+authors
    2. If query has a 4-digit year, it should match (with 1 year tolerance)
    
    FIX for Bug #2: Added 2025-12-09
    """
    if not book_dict:
        return False
    
    query_lower = query.lower()
    
    # Build searchable text from book result
    title = book_dict.get('title', '')
    authors = book_dict.get('authors', [])
    searchable = title.lower()
    if authors:
        searchable += ' ' + ' '.join(authors).lower()
    
    # Common words to ignore
    stop_words = {'the', 'a', 'an', 'of', 'and', 'in', 'on', 'at', 'to', 'for', 'by', 'with'}
    
    # Extract meaningful words from query (3+ chars, not stop words, not year)
    query_words = [w for w in query_lower.split() 
                   if len(w) >= 3 and w not in stop_words and not w.isdigit()]
    
    if not query_words:
        return True  # Nothing to match against
    
    # Count how many query words appear in searchable text
    matches = sum(1 for word in query_words if word in searchable)
    
    # Require at least 2 matches, OR all matches if only 1-2 words
    min_required = min(2, len(query_words))
    word_match = matches >= min_required
    
    # Check year if query contains one
    year_match = True  # Default to true if no year in query
    year_in_query = re.search(r'\b(19|20)\d{2}\b', query)
    book_year = book_dict.get('year', '')
    if year_in_query and book_year:
        query_year = int(year_in_query.group())
        try:
            result_year = int(str(book_year)[:4])
            # Allow 1 year tolerance for publication date variations
            year_match = abs(query_year - result_year) <= 1
        except (ValueError, TypeError):
            year_match = True  # Can't compare, allow it
    
    if word_match and year_match:
        print(f"[BookMatch] ✓ Matched {matches}/{len(query_words)} words: {query_words}")
    else:
        print(f"[BookMatch] ✗ Only {matches}/{len(query_words)} words matched in '{title[:50]}...'")
        print(f"[BookMatch]   Query words: {query_words}")
        print(f"[BookMatch]   Searchable: {searchable[:80]}...")
    
    return word_match and year_match


def _route_book(query: str) -> Optional[CitationMetadata]:
    """
    Route book queries using Cite Fix Pro's books.py.
    
    This is superior to CiteFlex Pro's google_cse.py because:
    1. Dual-engine: Open Library (precise ISBN) + Google Books (fuzzy search)
    2. PUBLISHER_PLACE_MAP fills in publication places
    3. ISBN detection routes to Open Library first
    
    UPDATED 2025-12-09: Added validation to prevent wrong book matches.
    """
    try:
        results = books.extract_metadata(query)
        if results and len(results) > 0:
            # Validate each result until we find one that matches
            for result in results:
                if _validate_book_match(query, result):
                    return _book_dict_to_metadata(result, query)
            
            # If no result matches, log warning and return None
            print(f"[UnifiedRouter] No book results matched query: {query[:50]}...")
            return None
    except Exception as e:
        print(f"[UnifiedRouter] Book search error: {e}")
    
    return None


# =============================================================================
# UNIFIED JOURNAL SEARCH (parallel execution)
# =============================================================================

def _titles_loosely_match(title1: str, title2: str) -> bool:
    """
    Check if two titles are similar enough to be the same work.
    Used to verify Crossref DOI matches Google Scholar result.
    
    Returns True if:
    - Titles are identical (case-insensitive)
    - One title contains the other (handles subtitles)
    - Word overlap is >= 60%
    """
    if not title1 or not title2:
        return False
    
    t1 = title1.lower().strip()
    t2 = title2.lower().strip()
    
    # Exact match
    if t1 == t2:
        return True
    
    # One contains the other (handles subtitles)
    if t1 in t2 or t2 in t1:
        return True
    
    # Word overlap ratio
    # Remove common punctuation
    import re
    t1_clean = re.sub(r'[^\w\s]', '', t1)
    t2_clean = re.sub(r'[^\w\s]', '', t2)
    
    words1 = set(t1_clean.split())
    words2 = set(t2_clean.split())
    
    # Remove very short words
    words1 = {w for w in words1 if len(w) > 2}
    words2 = {w for w in words2 if len(w) > 2}
    
    if not words1 or not words2:
        return False
    
    overlap = len(words1 & words2)
    smaller = min(len(words1), len(words2))
    
    return (overlap / smaller) >= 0.6


def _validate_journal_match(query: str, result: CitationMetadata) -> bool:
    """
    Validate that a journal/academic result matches the search query.
    
    Similar to book validation - prevents returning wrong articles like
    "Jieli Chen et al." for "Caplan trains brains".
    
    Requirements:
    1. At least 2 meaningful words from query must appear in title+authors
    2. If query has a 4-digit year, it should match (with 1 year tolerance)
    
    FIX for Bug: Added 2025-12-09
    """
    if not result or not result.title:
        return False
    
    query_lower = query.lower()
    
    # Build searchable text from result
    searchable = result.title.lower()
    if result.authors:
        searchable += ' ' + ' '.join(result.authors).lower()
    
    # Common words to ignore
    stop_words = {'the', 'a', 'an', 'of', 'and', 'in', 'on', 'at', 'to', 'for', 'by', 'with'}
    
    # Extract meaningful words from query (3+ chars, not stop words, not year)
    query_words = [w for w in query_lower.split() 
                   if len(w) >= 3 and w not in stop_words and not w.isdigit()]
    
    if not query_words:
        return True  # Nothing to match against
    
    # Count how many query words appear in searchable text
    matches = sum(1 for word in query_words if word in searchable)
    
    # Require at least 2 matches, OR all matches if only 1-2 words
    min_required = min(2, len(query_words))
    word_match = matches >= min_required
    
    # Check year if query contains one
    year_match = True  # Default to true if no year in query
    year_in_query = re.search(r'\b(19|20)\d{2}\b', query)
    if year_in_query and result.year:
        query_year = int(year_in_query.group())
        try:
            result_year = int(str(result.year)[:4])
            # Allow 1 year tolerance for publication date variations
            year_match = abs(query_year - result_year) <= 1
        except (ValueError, TypeError):
            year_match = True  # Can't compare, allow it
    
    if word_match and year_match:
        print(f"[JournalMatch] ✓ Matched {matches}/{len(query_words)} words: {query_words}")
    else:
        print(f"[JournalMatch] ✗ Only {matches}/{len(query_words)} words matched in '{result.title[:50]}...'")
    
    return word_match and year_match


def _route_journal(query: str) -> Optional[CitationMetadata]:
    """
    Route journal/academic queries using parallel API execution.
    
    Engines tried (in parallel):
    1. Crossref - best for DOIs, formal citations
    2. OpenAlex - good coverage, fast
    3. Semantic Scholar - good for author+title queries
    4. PubMed - medical/life sciences
    """
    # Check famous papers cache first (instant lookup for 10,000 most-cited)
    famous = find_famous_paper(query)
    if famous:
        try:
            result = _crossref.get_by_id(famous["doi"])
            if result:
                print("[UnifiedRouter] Found via Famous Papers cache")
                return result
        except Exception:
            pass
    
    # Check for DOI in query (instant lookup)
    doi_match = re.search(r'(10\.\d{4,}/[^\s]+)', query)
    if doi_match:
        doi = doi_match.group(1).rstrip('.,;')
        try:
            result = _crossref.get_by_id(doi)
            if result:
                print("[UnifiedRouter] Found via direct DOI lookup")
                return result
        except Exception:
            pass
    
    # Claude-first guess: Use Claude's knowledge to predict citation, then verify via APIs
    # This is especially effective for fragmentary queries like "Eric Caplan trains brains"
    try:
        claude_result = guess_and_search(query)
        if claude_result and claude_result.has_minimum_data():
            print(f"[UnifiedRouter] Found via Claude-first guess: {claude_result.source_engine}")
            return claude_result
    except Exception as e:
        print(f"[UnifiedRouter] Claude-first guess failed: {e}")
    
    # Parallel search across academic engines (fallback)
    results = []
    
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = {
            executor.submit(_crossref.search, query): "Crossref",
            executor.submit(_openalex.search, query): "OpenAlex",
            executor.submit(_semantic.search, query): "Semantic Scholar",
            executor.submit(_pubmed.search, query): "PubMed",
            executor.submit(_google_scholar.search, query): "Google Scholar",
            executor.submit(_arxiv.search, query): "arXiv",
        }
        
        for future in as_completed(futures, timeout=PARALLEL_TIMEOUT):
            engine_name = futures[future]
            try:
                result = future.result(timeout=2)
                if result and result.has_minimum_data():
                    # FIX 2025-12-09: Validate result matches query before accepting
                    if _validate_journal_match(query, result):
                        result.source_engine = engine_name
                        results.append(result)
            except Exception:
                pass
    
    # Enrich Google Scholar results with DOI from Crossref
    for result in results:
        if result.source_engine == "Google Scholar" and not result.doi:
            try:
                # Build search query from title + first author
                search_terms = result.title[:100] if result.title else ""
                if result.authors and len(result.authors) > 0:
                    # Get last name of first author
                    first_author = result.authors[0].split()[-1]
                    search_terms = f"{first_author} {search_terms}"
                
                if search_terms:
                    crossref_match = _crossref.search(search_terms)
                    if crossref_match and crossref_match.doi:
                        # Verify titles match before accepting DOI
                        if _titles_loosely_match(result.title, crossref_match.title):
                            result.doi = crossref_match.doi
                            result.source_engine = "Google Scholar + Crossref"
                            print(f"[UnifiedRouter] Enriched Google Scholar with DOI: {result.doi}")
            except Exception as e:
                print(f"[UnifiedRouter] DOI enrichment failed: {e}")
    
    # Return best result (prefer one with DOI)
    if results:
        for r in results:
            if r.doi:
                return r
        return results[0]
    
    return None


# =============================================================================
# URL ROUTING
# =============================================================================

def _is_medical_url(url: str) -> bool:
    """Check if URL is a medical resource (PubMed, NIH, etc.)."""
    url_lower = url.lower()
    return any(domain in url_lower for domain in MEDICAL_DOMAINS)


def _route_url(url: str) -> Optional[CitationMetadata]:
    """
    Route URL-based queries.
    
    Priority:
    1. Extract DOI from URL → Crossref lookup
    2. Academic publisher URL → Crossref search
    3. Medical URL → PubMed
    4. Newspaper URL → Newspaper extractor + Claude fallback
    5. Government URL → Government extractor + Claude fallback
    6. Generic URL → Basic metadata extraction + Claude fallback
    
    UPDATED 2025-12-09: Added newspaper/government detection and Claude fallback
    """
    from urllib.parse import urlparse
    
    # Check for DOI in URL
    doi = extract_doi_from_url(url)
    if doi:
        try:
            result = _crossref.get_by_id(doi)
            if result and result.has_minimum_data():
                result.url = url
                return result
        except Exception:
            pass
    
    # Fallback: Try generic DOI extraction from URL path
    doi_match = re.search(r'(10\.\d{4,}/[^\s?#]+)', url)
    if doi_match:
        doi = doi_match.group(1).rstrip('.,;')
        try:
            result = _crossref.get_by_id(doi)
            if result and result.has_minimum_data():
                result.url = url
                print("[UnifiedRouter] Found via DOI in URL path")
                return result
        except Exception:
            pass
    
    # Check for academic publisher
    if is_academic_publisher_url(url):
        try:
            result = _crossref.search(url)
            if result and result.has_minimum_data():
                result.url = url
                return result
        except Exception:
            pass
    
    # Medical URLs go to PubMed
    if _is_medical_url(url):
        try:
            result = _pubmed.search(url)
            if result and result.has_minimum_data():
                result.url = url
                return result
        except Exception:
            pass
    
    # =========================================================================
    # ArXiv URLs
    # =========================================================================
    if 'arxiv.org' in url.lower():
        print(f"[UnifiedRouter] Detected arXiv URL")
        try:
            result = _arxiv.search(url)
            if result and result.has_minimum_data():
                return result
        except Exception as e:
            print(f"[UnifiedRouter] arXiv lookup failed: {e}")
    
    # =========================================================================
    # YouTube/Vimeo video URLs
    # =========================================================================
    url_lower = url.lower()
    if 'youtube.com' in url_lower or 'youtu.be' in url_lower:
        print(f"[UnifiedRouter] Detected YouTube URL")
        try:
            result = _youtube.search(url)
            if result and result.has_minimum_data():
                return result
        except Exception as e:
            print(f"[UnifiedRouter] YouTube lookup failed: {e}")
    
    if 'vimeo.com' in url_lower:
        print(f"[UnifiedRouter] Detected Vimeo URL")
        try:
            result = _vimeo.search(url)
            if result and result.has_minimum_data():
                return result
        except Exception as e:
            print(f"[UnifiedRouter] Vimeo lookup failed: {e}")
    
    # =========================================================================
    # FIX 2025-12-09: Check newspaper and government domains
    # =========================================================================
    
    try:
        parsed = urlparse(url)
        domain = parsed.netloc.lower().replace('www.', '')
    except:
        domain = ""
    
    # Check for newspaper domains
    newspaper_domains = [
        'nytimes.com', 'washingtonpost.com', 'wsj.com', 'latimes.com',
        'theguardian.com', 'bbc.com', 'bbc.co.uk', 'reuters.com', 'apnews.com',
        'theatlantic.com', 'newyorker.com', 'economist.com', 'ft.com',
        'politico.com', 'axios.com', 'vox.com', 'slate.com', 'salon.com',
        'huffpost.com', 'buzzfeednews.com', 'vice.com', 'thedailybeast.com',
        'usatoday.com', 'chicagotribune.com', 'bostonglobe.com', 'sfchronicle.com',
        'nypost.com', 'nydailynews.com', 'newsweek.com', 'time.com',
        'forbes.com', 'businessinsider.com', 'cnbc.com', 'bloomberg.com',
        'cnn.com', 'foxnews.com', 'msnbc.com', 'nbcnews.com', 'cbsnews.com',
        'abcnews.go.com', 'npr.org', 'pbs.org', 'harpers.org', 'theroot.com',
    ]
    
    is_newspaper = any(news in domain for news in newspaper_domains)
    
    # Check for government domains (UK, US, etc.)
    government_patterns = [
        'gov.uk', 'nhs.uk', 'nice.org.uk', 'parliament.uk',
        'gov.scot', 'gov.wales', '.gov', 'gc.ca', 'canada.ca',
        'gov.au', 'govt.nz', 'gov.ie', 'europa.eu',
    ]
    
    is_government = any(pattern in domain for pattern in government_patterns)
    
    # Route to appropriate extractor
    if is_newspaper:
        print(f"[UnifiedRouter] Detected newspaper URL: {domain}")
        result = extract_by_type(url, CitationType.NEWSPAPER)
        if result and _has_good_url_metadata(result, url):
            return result
        # Try Claude fallback
        claude_result = _claude_url_fallback(url, CitationType.NEWSPAPER)
        if claude_result:
            return claude_result
        return result  # Return whatever we got
    
    if is_government:
        print(f"[UnifiedRouter] Detected government URL: {domain}")
        result = extract_by_type(url, CitationType.GOVERNMENT)
        if result and _has_good_url_metadata(result, url):
            return result
        # Try Claude fallback
        claude_result = _claude_url_fallback(url, CitationType.GOVERNMENT)
        if claude_result:
            return claude_result
        return result  # Return whatever we got
    
    # Fallback to standard extraction with Claude fallback
    result = extract_by_type(url, CitationType.URL)
    if result and _has_good_url_metadata(result, url):
        return result
    
    # Try Claude fallback for unknown URLs
    claude_result = _claude_url_fallback(url, CitationType.URL)
    if claude_result:
        return claude_result
    
    return result


def _has_good_url_metadata(result: CitationMetadata, url: str) -> bool:
    """Check if URL extraction returned meaningful data."""
    if not result:
        return False
    
    # No title at all
    if not result.title:
        return False
    
    # FIX 2025-12-09: For newspaper URLs, ALWAYS require an author
    # This ensures Claude fallback is triggered for paywalled/blocked sites
    if result.citation_type == CitationType.NEWSPAPER:
        has_author = result.authors and len(result.authors) > 0
        if not has_author:
            print(f"[UnifiedRouter] Newspaper URL has no author, triggering Claude fallback")
            return False
    
    try:
        from urllib.parse import urlparse
        parsed = urlparse(url)
        domain = parsed.netloc.lower().replace('www.', '')
        domain_base = domain.split('.')[0]  # e.g., "theatlantic" from "theatlantic.com"
    except:
        return True  # Can't parse, assume OK
    
    title_lower = result.title.lower().strip()
    title_normalized = title_lower.replace(' ', '').replace('-', '').replace('the', '')
    domain_normalized = domain_base.replace('the', '')
    
    # Title is just the domain/publication name (with or without spaces/articles)
    if title_normalized == domain_normalized:
        print(f"[UnifiedRouter] Title matches domain name, triggering Claude fallback")
        return False
    
    # Title is very short (likely just a site name)
    if len(result.title) < 15:
        print(f"[UnifiedRouter] Title too short ({len(result.title)} chars), triggering Claude fallback")
        return False
    
    return True


def _claude_url_fallback(url: str, citation_type: CitationType) -> Optional[CitationMetadata]:
    """
    Fast URL citation lookup using Brave Search API (preferred) or Claude.
    
    Priority:
    1. Brave Search API (~1-2 sec) - if BRAVE_API_KEY is set
    2. Claude + web search (~5-15 sec) - fallback
    3. URL path extraction (<1 sec) - last resort
    
    FIX 2025-12-09: Added Brave Search for ~5x faster lookups.
    """
    
    # =========================================================================
    # OPTION 1: Brave Search (fastest - ~1-2 sec)
    # =========================================================================
    try:
        from engines.brave_search import search_url_fallback, BRAVE_API_KEY
        if BRAVE_API_KEY:
            print(f"[UnifiedRouter] Using Brave Search for URL: {url[:50]}...")
            result = search_url_fallback(url, citation_type)
            if result and result.title and len(result.title) > 10:
                return result
            print(f"[UnifiedRouter] Brave Search returned no/minimal result")
    except ImportError:
        pass  # Brave Search not available
    except Exception as e:
        print(f"[UnifiedRouter] Brave Search error: {e}")
    
    # =========================================================================
    # OPTION 2: Claude + Web Search (slower - ~5-15 sec)
    # =========================================================================
    try:
        from routers.claude import _get_client, CLAUDE_MODEL
        
        client = _get_client()
        if client:
            print(f"[UnifiedRouter] Using Claude web search for URL: {url[:50]}...")
            
            # Extract search terms from URL
            from urllib.parse import urlparse
            parsed = urlparse(url)
            domain = parsed.netloc.lower().replace('www.', '')
            path = parsed.path.strip('/')
            
            slug_parts = []
            for seg in path.split('/'):
                if '-' in seg and not seg.isdigit() and len(seg) > 5:
                    slug_parts.append(seg.replace('-', ' '))
            
            search_query = f"{domain.split('.')[0]} {' '.join(slug_parts)}"
            
            prompt = f"""I need citation information for this URL: {url}

Search for this article and provide the citation metadata. Search query hint: {search_query}

After searching, respond with ONLY this JSON (no other text):
{{
    "title": "Full article title",
    "authors": ["Author Name"],
    "date": "Month DD, YYYY",
    "publication": "Publication name",
    "confidence": 1.0
}}"""

            response = client.messages.create(
                model=CLAUDE_MODEL,
                max_tokens=500,
                tools=[{
                    "type": "web_search_20250305",
                    "name": "web_search"
                }],
                messages=[{"role": "user", "content": prompt}]
            )
            
            text = ""
            for block in response.content:
                if hasattr(block, 'text'):
                    text += block.text
            
            import json
            json_match = re.search(r'\{[\s\S]*\}', text)
            if json_match:
                data = json.loads(json_match.group())
                
                title = data.get('title', '')
                if title and len(title) > 10 and data.get('confidence', 0) >= 0.5:
                    print(f"[UnifiedRouter] Claude found: {title[:50]}...")
                    
                    from datetime import datetime
                    access_date = datetime.now().strftime('%B %d, %Y').replace(' 0', ' ')
                    
                    return CitationMetadata(
                        citation_type=citation_type,
                        raw_source=url,
                        source_engine="Claude Web Search",
                        url=url,
                        title=title,
                        authors=data.get('authors', []),
                        date=data.get('date', ''),
                        newspaper=data.get('publication', '') if citation_type == CitationType.NEWSPAPER else None,
                        agency=data.get('publication', '') if citation_type == CitationType.GOVERNMENT else None,
                        access_date=access_date,
                    )
                    
    except ImportError:
        pass
    except Exception as e:
        print(f"[UnifiedRouter] Claude web search error: {e}")
    
    # =========================================================================
    # OPTION 3: URL Path Extraction (last resort)
    # =========================================================================
    return _extract_from_url_path(url, citation_type)


def _extract_from_url_path(url: str, citation_type: CitationType) -> Optional[CitationMetadata]:
    """
    Extract title from URL path when all else fails.
    
    Converts slugs like "private-equity-housing-changes" to readable titles.
    """
    from urllib.parse import urlparse
    from datetime import datetime
    
    try:
        parsed = urlparse(url)
        domain = parsed.netloc.lower().replace('www.', '')
        path = parsed.path.strip('/')
        
        if not path:
            return None
        
        # Split path into segments
        segments = path.split('/')
        
        # Find the best segment for title (longest slug with words)
        best_slug = None
        for seg in reversed(segments):
            # Skip numeric-only segments (like "685138")
            if seg.isdigit():
                continue
            # Skip very short segments
            if len(seg) < 5:
                continue
            # Skip date-like segments
            if re.match(r'^\d{4}$', seg) or re.match(r'^\d{2}$', seg):
                continue
            # This looks like a good slug
            if '-' in seg or '_' in seg:
                best_slug = seg
                break
        
        if not best_slug:
            return None
        
        # Convert slug to title
        title = best_slug.replace('-', ' ').replace('_', ' ')
        # Title case, but preserve common acronyms
        title = title.title()
        
        # Fix common acronyms
        acronym_fixes = {
            'Ai': 'AI', 'Us': 'US', 'Uk': 'UK', 'Fda': 'FDA', 'Nih': 'NIH',
            'Cdc': 'CDC', 'Ceo': 'CEO', 'Covid': 'COVID', 'Nhs': 'NHS',
        }
        for wrong, right in acronym_fixes.items():
            title = re.sub(r'\b' + wrong + r'\b', right, title)
        
        print(f"[UnifiedRouter] Extracted title from URL path: {title}")
        
        # Try to extract date from URL path (e.g., /2025/12/)
        date_match = re.search(r'/(\d{4})/(\d{1,2})/', url)
        date_str = ""
        if date_match:
            year, month = date_match.groups()
            try:
                from calendar import month_name
                date_str = f"{month_name[int(month)]} {year}"
            except:
                date_str = f"{year}"
        
        # Get publication name from domain
        domain_base = domain.split('.')[0]
        publication = domain_base.replace('the', '').title()
        if domain_base == 'theatlantic':
            publication = 'The Atlantic'
        elif domain_base == 'nytimes':
            publication = 'New York Times'
        elif domain_base == 'washingtonpost':
            publication = 'Washington Post'
        
        access_date = datetime.now().strftime('%B %d, %Y').replace(' 0', ' ')
        
        return CitationMetadata(
            citation_type=citation_type,
            raw_source=url,
            source_engine="URL Path Extraction",
            url=url,
            title=title,
            authors=[],  # Can't get author from URL
            date=date_str,
            newspaper=publication if citation_type == CitationType.NEWSPAPER else None,
            access_date=access_date,
        )
        
    except Exception as e:
        print(f"[UnifiedRouter] URL path extraction error: {e}")
        return None


# =============================================================================
# MAIN ROUTING FUNCTION
# =============================================================================

def route_citation(query: str, style: str = "chicago") -> Tuple[Optional[CitationMetadata], str]:
    """
    Main entry point: route query to appropriate engine and format result.
    
    Returns: (CitationMetadata, formatted_citation_string)
    
    NEW (V3.4): Tries to parse already-formatted citations first.
    If the citation is complete (has author, title, journal/publisher, year),
    it reformats without searching databases. This preserves authoritative
    content while applying consistent style formatting.
    """
    query = query.strip()
    if not query:
        return None, ""
    
    formatter = get_formatter(style)
    metadata = None
    
    # 0. TRY PARSING FIRST: If citation is already complete, just reformat
    # This preserves user's authoritative content while applying style
    parsed = parse_existing_citation(query)
    if parsed and _is_citation_complete(parsed):
        print(f"[UnifiedRouter] Parsed complete citation: {parsed.citation_type.name}")
        return parsed, formatter.format(parsed)
    
    # 1. Check for legal citation FIRST (legal.py handles famous cases)
    if legal.is_legal_citation(query):
        metadata = _route_legal(query)
        if metadata:
            return metadata, formatter.format(metadata)
    
    # 2. Check for URL
    if is_url(query):
        metadata = _route_url(query)
        if metadata:
            return metadata, formatter.format(metadata)
    
    # 2.5. Claude-first guess: Use Claude's knowledge for ambiguous queries
    # This catches fragmentary queries like "Eric Caplan trains brains" that
    # detectors might misroute to books instead of journals
    try:
        claude_result = guess_and_search(query)
        if claude_result and claude_result.has_minimum_data():
            print(f"[UnifiedRouter] Found via Claude-first guess: {claude_result.source_engine}")
            return claude_result, formatter.format(claude_result)
    except Exception as e:
        print(f"[UnifiedRouter] Claude-first guess failed: {e}")
    
    # 3. Detect type using standard detectors
    detection = detect_type(query)
    
    # 4. Route based on detection
    if detection.citation_type == CitationType.LEGAL:
        metadata = _route_legal(query)
    
    elif detection.citation_type == CitationType.BOOK:
        metadata = _route_book(query)
        # FIX 2025-12-09: If book search fails (validation rejected or not found),
        # try journal engines as fallback. Fragmentary queries like "Caplan trains brains"
        # might be journal articles misdetected as books.
        if not metadata:
            print(f"[UnifiedRouter] Book search failed, trying journal engines...")
            metadata = _route_journal(query)
    
    elif detection.citation_type in [CitationType.JOURNAL, CitationType.MEDICAL]:
        # Check famous papers cache first
        famous = find_famous_paper(query)
        if famous:
            metadata = CitationMetadata(
                citation_type=CitationType.JOURNAL,
                raw_source=query,
                source_engine="Famous Papers Cache",
                **famous
            )
        else:
            metadata = _route_journal(query)
    
    elif detection.citation_type == CitationType.NEWSPAPER:
        metadata = extract_by_type(query, CitationType.NEWSPAPER)
    
    elif detection.citation_type == CitationType.GOVERNMENT:
        metadata = extract_by_type(query, CitationType.GOVERNMENT)
    
    elif detection.citation_type == CitationType.INTERVIEW:
        metadata = extract_by_type(query, CitationType.INTERVIEW)
    
    else:
        # UNKNOWN: Try AI classification first
        if AI_AVAILABLE:
            ai_type, ai_meta = classify_with_ai(query)
            if ai_type != CitationType.UNKNOWN:
                print(f"[UnifiedRouter] AI classified as: {ai_type.name}")
                
                if ai_type == CitationType.BOOK:
                    metadata = _route_book(query)
                elif ai_type == CitationType.LEGAL:
                    metadata = _route_legal(query)
                elif ai_type in [CitationType.JOURNAL, CitationType.MEDICAL]:
                    metadata = _route_journal(query)
                elif ai_type == CitationType.NEWSPAPER:
                    metadata = extract_by_type(query, CitationType.NEWSPAPER)
                elif ai_type == CitationType.GOVERNMENT:
                    metadata = extract_by_type(query, CitationType.GOVERNMENT)
        
        # Fallback: try books first, then journals
        if not metadata:
            metadata = _route_book(query)
        if not metadata:
            metadata = _route_journal(query)
    
    # Format and return
    if metadata:
        return metadata, formatter.format(metadata)
    
    return None, ""


# =============================================================================
# MULTIPLE RESULTS FUNCTION
# =============================================================================

def get_multiple_citations(query: str, style: str = "chicago", limit: int = 5) -> List[Tuple[CitationMetadata, str, str]]:
    """
    Get multiple citation candidates for user selection.
    
    Returns list of (metadata, formatted_citation, source_name) tuples.
    
    NEW (V3.4): If citation is already complete, returns parsed version first
    as "Original (Reformatted)" before database results.
    """
    query = query.strip()
    if not query:
        return []
    
    formatter = get_formatter(style)
    results = []
    
    # TRY PARSING FIRST: If citation is complete, show reformatted version first
    parsed = parse_existing_citation(query)
    if parsed and _is_citation_complete(parsed):
        formatted = formatter.format(parsed)
        results.append((parsed, formatted, "Original (Reformatted)"))
        print(f"[UnifiedRouter] Parsed complete citation, added as first option")
    
    # Detect type
    detection = detect_type(query)
    
    # Check for URL with DOI first
    if is_url(query):
        doi = extract_doi_from_url(query)
        if doi:
            try:
                result = _crossref.get_by_id(doi)
                if result and result.has_minimum_data():
                    result.url = query
                    formatted = formatter.format(result)
                    results.append((result, formatted, "Crossref (DOI)"))
            except Exception:
                pass
    
    # Check for legal citation
    if legal.is_legal_citation(query) or detection.citation_type == CitationType.LEGAL:
        metadata = _route_legal(query)
        if metadata:
            formatted = formatter.format(metadata)
            results.append((metadata, formatted, "Legal Cache"))
        return results  # Legal citations typically have one authoritative result
    
    # For journals/academic
    if detection.citation_type in [CitationType.JOURNAL, CitationType.MEDICAL, CitationType.UNKNOWN]:
        # Check famous papers first
        famous = find_famous_paper(query)
        if famous:
            meta = CitationMetadata(
                citation_type=CitationType.JOURNAL,
                raw_source=query,
                source_engine="Famous Papers Cache",
                **famous
            )
            formatted = formatter.format(meta)
            results.append((meta, formatted, "Famous Papers"))
        
        # Query multiple engines
        try:
            metadatas = _crossref.search_multiple(query, limit)
            for meta in metadatas:
                if meta and meta.has_minimum_data():
                    # FIX 2025-12-09: Validate result matches query
                    if _validate_journal_match(query, meta):
                        formatted = formatter.format(meta)
                        results.append((meta, formatted, "Crossref"))
        except Exception:
            pass
        
        # Add Semantic Scholar results
        if len(results) < limit:
            try:
                ss_result = _semantic.search(query)
                if ss_result and ss_result.has_minimum_data():
                    # FIX 2025-12-09: Validate result matches query
                    if _validate_journal_match(query, ss_result):
                        is_duplicate = any(
                            ss_result.title and r[0].title and 
                            ss_result.title.lower()[:30] == r[0].title.lower()[:30]
                            for r in results
                        )
                        if not is_duplicate:
                            formatted = formatter.format(ss_result)
                            results.append((ss_result, formatted, "Semantic Scholar"))
            except Exception:
                pass
        
        # Also search book engines (Google Books, Library of Congress, Open Library)
        # Many queries could be books misclassified as journals
        if len(results) < limit:
            try:
                book_results = books.search_all_engines(query)
                for data in book_results:
                    if len(results) >= limit:
                        break
                    # FIX 2025-12-09: Validate book matches query before adding
                    if not _validate_book_match(query, data):
                        continue
                    meta = _book_dict_to_metadata(data, query)
                    if meta and meta.has_minimum_data():
                        # Check for duplicates
                        is_duplicate = any(
                            meta.title and r[0].title and 
                            meta.title.lower()[:30] == r[0].title.lower()[:30]
                            for r in results
                        )
                        if not is_duplicate:
                            formatted = formatter.format(meta)
                            source = data.get('source_engine', 'Google Books')
                            results.append((meta, formatted, source))
            except Exception as e:
                print(f"[UnifiedRouter] Book engines error: {e}")
            except Exception:
                pass
    
    elif detection.citation_type == CitationType.BOOK:
        # Query ALL book engines (Google Books, Library of Congress, Open Library)
        try:
            book_results = books.search_all_engines(query)
            for data in book_results:
                if len(results) >= limit:
                    break
                # FIX 2025-12-09: Validate book matches query before adding
                if not _validate_book_match(query, data):
                    continue
                meta = _book_dict_to_metadata(data, query)
                if meta and meta.has_minimum_data():
                    # Check for duplicates
                    is_duplicate = any(
                        meta.title and r[0].title and 
                        meta.title.lower()[:30] == r[0].title.lower()[:30]
                        for r in results
                    )
                    if not is_duplicate:
                        formatted = formatter.format(meta)
                        source = data.get('source_engine', 'Google Books')
                        results.append((meta, formatted, source))
        except Exception as e:
            print(f"[UnifiedRouter] Book engines error: {e}")
        
        # Also try Crossref (has book chapters)
        if len(results) < limit:
            try:
                metadatas = _crossref.search_multiple(query, limit - len(results))
                for meta in metadatas:
                    if meta and meta.has_minimum_data():
                        formatted = formatter.format(meta)
                        results.append((meta, formatted, "Crossref"))
            except Exception:
                pass
        
        # Also try Semantic Scholar
        if len(results) < limit:
            try:
                ss_result = _semantic.search(query)
                if ss_result and ss_result.has_minimum_data():
                    is_duplicate = any(
                        ss_result.title and r[0].title and 
                        ss_result.title.lower()[:30] == r[0].title.lower()[:30]
                        for r in results
                    )
                    if not is_duplicate:
                        formatted = formatter.format(ss_result)
                        results.append((ss_result, formatted, "Semantic Scholar"))
            except Exception:
                pass
    
    elif detection.citation_type == CitationType.UNKNOWN:
        # Try AI router to classify ambiguous queries
        if AI_AVAILABLE:
            ai_type, ai_meta = classify_with_ai(query)
            if ai_type != CitationType.UNKNOWN:
                print(f"[UnifiedRouter] AI classified as: {ai_type.name}")
                
                # Route based on AI's classification
                if ai_type == CitationType.BOOK:
                    try:
                        book_results = books.search_all_engines(query)
                        for data in book_results:
                            if len(results) >= limit:
                                break
                            meta = _book_dict_to_metadata(data, query)
                            if meta and meta.has_minimum_data():
                                is_duplicate = any(
                                    meta.title and r[0].title and 
                                    meta.title.lower()[:30] == r[0].title.lower()[:30]
                                    for r in results
                                )
                                if not is_duplicate:
                                    formatted = formatter.format(meta)
                                    source = data.get('source_engine', 'Google Books')
                                    results.append((meta, formatted, source))
                    except Exception as e:
                        print(f"[UnifiedRouter] Book engines error: {e}")
                    # Also try Semantic Scholar
                    if len(results) < limit:
                        try:
                            ss_result = _semantic.search(query)
                            if ss_result and ss_result.has_minimum_data():
                                is_duplicate = any(
                                    ss_result.title and r[0].title and 
                                    ss_result.title.lower()[:30] == r[0].title.lower()[:30]
                                    for r in results
                                )
                                if not is_duplicate:
                                    formatted = formatter.format(ss_result)
                                    results.append((ss_result, formatted, "Semantic Scholar"))
                        except Exception:
                            pass
                    return results[:limit]
                
                elif ai_type == CitationType.LEGAL:
                    metadata = _route_legal(query)
                    if metadata:
                        formatted = formatter.format(metadata)
                        results.append((metadata, formatted, "Legal Cache"))
                    return results
                
                elif ai_type in [CitationType.JOURNAL, CitationType.MEDICAL]:
                    metadatas = _crossref.search_multiple(query, limit)
                    for meta in metadatas:
                        if meta and meta.has_minimum_data():
                            formatted = formatter.format(meta)
                            results.append((meta, formatted, "Crossref"))
                    # Also try Semantic Scholar
                    if len(results) < limit:
                        try:
                            ss_result = _semantic.search(query)
                            if ss_result and ss_result.has_minimum_data():
                                is_duplicate = any(
                                    ss_result.title and r[0].title and 
                                    ss_result.title.lower()[:30] == r[0].title.lower()[:30]
                                    for r in results
                                )
                                if not is_duplicate:
                                    formatted = formatter.format(ss_result)
                                    results.append((ss_result, formatted, "Semantic Scholar"))
                        except Exception:
                            pass
                    # Also try book engines (could be a book, not just journal)
                    if len(results) < limit:
                        try:
                            book_results = books.search_all_engines(query)
                            for data in book_results:
                                if len(results) >= limit:
                                    break
                                meta = _book_dict_to_metadata(data, query)
                                if meta and meta.has_minimum_data():
                                    is_duplicate = any(
                                        meta.title and r[0].title and 
                                        meta.title.lower()[:30] == r[0].title.lower()[:30]
                                        for r in results
                                    )
                                    if not is_duplicate:
                                        formatted = formatter.format(meta)
                                        source = data.get('source_engine', 'Google Books')
                                        results.append((meta, formatted, source))
                        except Exception:
                            pass
                    return results[:limit]
        
        # Fallback: try ALL book engines (often what users want)
        try:
            book_results = books.search_all_engines(query)
            for data in book_results:
                if len(results) >= limit:
                    break
                meta = _book_dict_to_metadata(data, query)
                if meta and meta.has_minimum_data():
                    is_duplicate = any(
                        meta.title and r[0].title and 
                        meta.title.lower()[:30] == r[0].title.lower()[:30]
                        for r in results
                    )
                    if not is_duplicate:
                        formatted = formatter.format(meta)
                        source = data.get('source_engine', 'Google Books')
                        results.append((meta, formatted, source))
        except Exception as e:
            print(f"[UnifiedRouter] Book engines error: {e}")
        
        # Then fill remaining with Crossref (journals, chapters)
        if len(results) < limit:
            try:
                metadatas = _crossref.search_multiple(query, limit - len(results))
                for meta in metadatas:
                    if meta and meta.has_minimum_data():
                        formatted = formatter.format(meta)
                        results.append((meta, formatted, "Crossref"))
            except Exception:
                pass
        
        # Finally try Semantic Scholar
        if len(results) < limit:
            try:
                ss_result = _semantic.search(query)
                if ss_result and ss_result.has_minimum_data():
                    is_duplicate = any(
                        ss_result.title and r[0].title and 
                        ss_result.title.lower()[:30] == r[0].title.lower()[:30]
                        for r in results
                    )
                    if not is_duplicate:
                        formatted = formatter.format(ss_result)
                        results.append((ss_result, formatted, "Semantic Scholar"))
            except Exception:
                pass
    
    return results[:limit]


# =============================================================================
# MULTI-OPTION CITATIONS (uses Claude's get_citation_options)
# =============================================================================

def get_citation_options_formatted(query: str, style: str = "chicago", limit: int = 5) -> List[dict]:
    """
    Get multiple citation options using Claude AI + multiple APIs.
    
    This is the preferred method for ambiguous queries like "Caplan mind games".
    Returns list of dicts with {citation, source, title, authors, year, ...}.
    
    Uses claude_router.get_citation_options() which searches:
    - Google Books
    - Crossref  
    - PubMed
    - Famous Cases Cache
    """
    if CLAUDE_AVAILABLE:
        try:
            return get_citation_options(query, max_options=limit)
        except Exception as e:
            print(f"[UnifiedRouter] Claude options error: {e}")
    
    # Fallback to standard multiple citations
    results = get_multiple_citations(query, style, limit)
    return [
        {
            "citation": formatted,
            "source": source,
            "title": meta.title if meta else "",
            "authors": meta.authors if meta else [],
            "year": meta.year if meta else ""
        }
        for meta, formatted, source in results
    ]


# =============================================================================
# BACKWARD COMPATIBILITY
# =============================================================================

# Alias for app.py compatibility
def get_citation(query: str, style: str = "chicago") -> Tuple[Optional[CitationMetadata], str]:
    """Alias for route_citation() - backward compatibility."""
    return route_citation(query, style)


def search_citation(query: str) -> List[dict]:
    """
    Backward-compatible search function.
    Returns list of dicts (matching old search.py interface).
    """
    results = []
    
    # Try legal first
    if legal.is_legal_citation(query):
        data = legal.extract_metadata(query)
        if data:
            results.append(data)
        return results
    
    # Try books
    try:
        book_results = books.extract_metadata(query)
        results.extend(book_results)
    except Exception:
        pass
    
    # Try academic
    try:
        meta = _route_journal(query)
        if meta:
            results.append(meta.to_dict())
    except Exception:
        pass
    
    return results
